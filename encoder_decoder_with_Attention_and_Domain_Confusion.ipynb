{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder-decoder-withDC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wzEeh8OFLhF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "080f5cd8-7509-4bc8-cdf6-de402be63a4c"
      },
      "source": [
        "pip install --upgrade soundfile librosa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/68/64/1191352221e2ec90db7492b4bf0c04fd9d2508de67b3f39cbf093cd6bd86/SoundFile-0.10.2-py2.py3-none-any.whl\n",
            "Collecting librosa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/71/c21ccef81276d85b9e3a36d80dad5baaf8a91f912e65eff0fd0d74a5a19c/librosa-0.7.1.tar.gz (1.6MB)\n",
            "\r\u001b[K     |▏                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 32.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 38.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 13.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92kB 20.6MB/s eta 0:00:01\r\u001b[K     |██                              | 102kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143kB 22.1MB/s eta 0:00:01\r\u001b[K     |███                             | 153kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184kB 22.1MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 22.1MB/s eta 0:00:01\r\u001b[K     |████                            | 204kB 22.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215kB 22.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 225kB 22.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 235kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 245kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 256kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 276kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 286kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 296kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 307kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 317kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 327kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 337kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 348kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 358kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 368kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 378kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 389kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 399kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 409kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 419kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 430kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 440kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 450kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 460kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 471kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 481kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 491kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 512kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 522kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 532kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 542kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 552kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 563kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 573kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 583kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 593kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 604kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 614kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 624kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 634kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 645kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 655kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 665kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 675kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 686kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 696kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 706kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 716kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 727kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 737kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 747kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 757kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 768kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 778kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 788kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 798kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 808kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 819kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 829kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 839kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 849kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 860kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 870kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 880kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 890kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 901kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 911kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 921kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 931kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 942kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 952kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 962kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 972kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 983kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 993kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.0MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.6MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.6MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.6MB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6MB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6MB 22.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.13.2)\n",
            "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Collecting numba>=0.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/34/22b6c2ded558976b5367be01b851ae679a0d1ba994de002d54afe84187b5/numba-0.46.0-cp36-cp36m-manylinux1_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.19)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite>=0.30.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa) (0.30.0)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.1-cp36-none-any.whl size=1610157 sha256=b11cab9d9f8a3caadac550547c65ae751362cb2a61216958c954e8cb24b61361\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/36/47/a9a4d151332cbdaec564500af9704a0ad862cf554dcf4bfda0\n",
            "Successfully built librosa\n",
            "Installing collected packages: soundfile, numba, librosa\n",
            "  Found existing installation: numba 0.40.1\n",
            "    Uninstalling numba-0.40.1:\n",
            "      Successfully uninstalled numba-0.40.1\n",
            "  Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "Successfully installed librosa-0.7.1 numba-0.46.0 soundfile-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JASTarLlFhFw"
      },
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import random\n",
        "\n",
        "y, sr = librosa.load('2342.wav', duration=5, sr=16000)\n",
        "# sf.write('2330_0.wav', y, sr)\n",
        "# ipd.Audio('2330_0.wav')\n",
        "D = np.abs(librosa.stft(y))**2\n",
        "S = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=64, fmax=8000)\n",
        "plt.subplot(121)\n",
        "S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram')\n",
        "plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "y_hat = librosa.feature.inverse.mel_to_audio(S, sr)\n",
        "# sf.write('2330_1.wav', y_hat, sr)\n",
        "# ipd.Audio('2330_1.wav')\n",
        "ipd.Audio(y_hat, rate=sr)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda:0'\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.GRU(input_dim, enc_hid_dim, n_layers, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = src\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        # print(\"Values returned by encoder: \", outputs.shape, hidden.shape)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat encoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "\n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return torch.nn.functional.softmax(attention, dim=1)\n",
        "        \n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "                \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + output_dim, dec_hid_dim, n_layers)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + output_dim, output_dim)\n",
        "        \n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # print(\"input.size in Decoder forward\", input.size())\n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = input\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "         #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        # print(\"Output.size() in Decoder forward: \", output.size())\n",
        "        return output, hidden.squeeze(0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        # print(\"max_len: \", max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        # print(\"Input.size in Seq2Seq forward: \", input.size())\n",
        "        for t in range(1, max_len):\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # print(\"Teacher forcing on? \", teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            # top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else output\n",
        "            # print(\"Input shape used for next pass to decoder: \", input.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "INPUT_DIM = 64\n",
        "OUTPUT_DIM = 64\n",
        "ENC_HID_DIM = 32\n",
        "DEC_HID_DIM = 32\n",
        "ENC_N_LAYERS = 4\n",
        "DEC_N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_N_LAYERS, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "x = torch.tensor(np.expand_dims(S.T, 1), dtype=torch.float)\n",
        "mu = x.mean(axis=0, keepdims=True)\n",
        "sig = x.std(axis=0, keepdims=True)\n",
        "x = (x-mu)/sig\n",
        "\n",
        "x = x.to(device)\n",
        "\n",
        "model.train()\n",
        "        \n",
        "for i in range(1000):\n",
        "    src = x\n",
        "    tgt = x\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, tgt)   \n",
        "    # print(\"Should be same\", output.shape, tgt.shape)\n",
        "\n",
        "    loss = criterion(output, tgt)        \n",
        "    loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 100)\n",
        "    optimizer.step()\n",
        "    if i%10 == 9:\n",
        "        # print(torch.abs(x[1]-output[1]))\n",
        "        print(i, loss.item())\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(x, x, 0) #turn off teacher forcing\n",
        "    loss = criterion(x, x)\n",
        "\n",
        "plt.subplot(122)\n",
        "output = output.detach().cpu()*sig + mu\n",
        "S_dB = librosa.power_to_db(output[:,0,:].T, ref=np.max)\n",
        "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "y_hat = librosa.feature.inverse.mel_to_audio(output.numpy()[:,0,:].T, sr)\n",
        "sf.write('2330_2.wav', y_hat, sr)\n",
        "# ipd.Audio('2330_2.wav')\n",
        "ipd.Audio(y_hat, rate=sr)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}